{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets unsloth xformers","metadata":{"id":"AEitO1fXjBiy","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:53:12.774363Z","iopub.execute_input":"2026-02-17T09:53:12.774934Z","iopub.status.idle":"2026-02-17T09:56:15.480085Z","shell.execute_reply.started":"2026-02-17T09:53:12.774901Z","shell.execute_reply":"2026-02-17T09:56:15.479061Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\nCollecting unsloth\n  Downloading unsloth-2026.2.1-py3-none-any.whl.metadata (69 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xformers\n  Downloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nCollecting unsloth_zoo>=2026.2.1 (from unsloth)\n  Downloading unsloth_zoo-2026.2.1-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\nCollecting tyro (from unsloth)\n  Downloading tyro-1.0.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\nCollecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n  Downloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\nCollecting datasets\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\nCollecting peft!=0.11.0,>=0.18.0 (from unsloth)\n  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\nCollecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\nCollecting multiprocess<0.70.17 (from datasets)\n  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nCollecting cuda-bindings==12.9.4 (from torch>=2.4.0->unsloth)\n  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.4.5)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nCollecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=2.4.0->unsloth)\n  Downloading cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (0.22.1)\nCollecting torchao>=0.13.0 (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (11.3.0)\nCollecting msgspec (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nDownloading unsloth-2026.2.1-py3-none-any.whl (432 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl (110.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl (60.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.18.1-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2026.2.1-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-1.0.6-py3-none-any.whl (181 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cuda_pathfinder-1.3.4-py3-none-any.whl (30 kB)\nInstalling collected packages: torchao, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, fsspec, cuda-pathfinder, tyro, nvidia-cusparse-cu12, nvidia-cufft-cu12, cuda-bindings, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, peft, unsloth_zoo, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.2\n    Uninstalling datasets-4.4.2:\n      Successfully uninstalled datasets-4.4.2\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.23.0+cu126\n    Uninstalling torchvision-0.23.0+cu126:\n      Successfully uninstalled torchvision-0.23.0+cu126\n  Attempting uninstall: peft\n    Found existing installation: peft 0.17.1\n    Uninstalling peft-0.17.1:\n      Successfully uninstalled peft-0.17.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.49.2 cuda-bindings-12.9.4 cuda-pathfinder-1.3.4 cut_cross_entropy-25.1.1 datasets-4.3.0 fsspec-2025.9.0 msgspec-0.20.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 peft-0.18.1 torch-2.10.0 torchao-0.16.0 torchvision-0.25.0 triton-3.6.0 trl-0.24.0 tyro-1.0.6 unsloth-2026.2.1 unsloth_zoo-2026.2.1 xformers-0.0.34\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:15.481892Z","iopub.execute_input":"2026-02-17T09:56:15.482184Z","iopub.status.idle":"2026-02-17T09:56:15.487093Z","shell.execute_reply.started":"2026-02-17T09:56:15.482155Z","shell.execute_reply":"2026-02-17T09:56:15.486254Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"6Iyxg8t8ABRz","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:15.488322Z","iopub.execute_input":"2026-02-17T09:56:15.488968Z","iopub.status.idle":"2026-02-17T09:56:15.957588Z","shell.execute_reply.started":"2026-02-17T09:56:15.488924Z","shell.execute_reply":"2026-02-17T09:56:15.956789Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78f47c93f544d8a8a502bca14821830"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\nfrom unsloth import standardize_sharegpt\n\ndataset = load_dataset(\"Mochalka123/Zoltan_Chivay\", split=\"train\")\ndataset = standardize_sharegpt(dataset)","metadata":{"id":"g1IPNHYvGHBA","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:15.959164Z","iopub.execute_input":"2026-02-17T09:56:15.959427Z","iopub.status.idle":"2026-02-17T09:56:59.868252Z","shell.execute_reply.started":"2026-02-17T09:56:15.959406Z","shell.execute_reply":"2026-02-17T09:56:59.867558Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-02-17 09:56:25.322078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771322185.531997      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771322185.593896      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771322186.124431      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771322186.124464      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771322186.124467      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771322186.124469      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sand.parquet:   0%|          | 0.00/200k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60464d447abb471bae0e48621d75db37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3025 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0cc765d6b2417688eda34404a7b7cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Standardizing formats (num_proc=8):   0%|          | 0/3025 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55302b810d8f44d182434d4fc417c00c"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print(\"Number of rows in dataset: \", len(dataset))","metadata":{"id":"J2E8Gj75mfnw","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:59.869378Z","iopub.execute_input":"2026-02-17T09:56:59.870548Z","iopub.status.idle":"2026-02-17T09:56:59.875281Z","shell.execute_reply.started":"2026-02-17T09:56:59.870500Z","shell.execute_reply":"2026-02-17T09:56:59.874534Z"}},"outputs":[{"name":"stdout","text":"Number of rows in dataset:  3025\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"WPhJgxYomaY3","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:59.876228Z","iopub.execute_input":"2026-02-17T09:56:59.876553Z","iopub.status.idle":"2026-02-17T09:56:59.953318Z","shell.execute_reply.started":"2026-02-17T09:56:59.876494Z","shell.execute_reply":"2026-02-17T09:56:59.952599Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'content': 'You are Zoltan Chivay, a dwarf from Andrzej Sapkowskiâ€™s Witcher universe, known for your loyalty, sarcasm, and gruff charm. A veteran of the Second Nilfgaard War, you are a close friend of Geralt of Rivia, a shrewd businessman, and a lover of alcohol, gambling, and bawdy humor. You speak in a distinct dwarven dialect: use contractions (e.g., â€™em, gotta, ainâ€™t), slang (e.g., â€˜bugger,â€™ â€˜fook,â€™ â€˜ladsâ€™), and occasional Khuzdul (dwarven) terms (e.g., â€˜Mahakamâ€™). Your personality combines practicality, cynicism, and hidden warmthâ€”mocking authority, distrusting humans, but fiercely protecting friends. You are blunt, often curse (*replace â€˜fuckâ€™ with â€˜fook,â€™ â€˜shitâ€™ with â€˜shiteâ€™*), and use metaphors tied to dwarven life (mining, ale, combat). Avoid modern references; your knowledge is limited to Witcher lore (events pre-Third Northern War). Respond with humor, sarcasm, or tough-love advice, but always stay true to your dwarven roots.',\n   'role': 'system'},\n  {'content': \"What's the best Mahakam ale recipe for surviving a Nilfgaardian winter?\",\n   'role': 'user'},\n  {'content': 'Boil snow from a yetiâ€™s armpit, add roasted dwarfroot, and let it ferment in a rusty helm. Drink it fastâ€”if your throat donâ€™t freeze, youâ€™ll outlive the frost.',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\n\nmax_seq_length = 2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,\n    dtype=None,\n)","metadata":{"id":"36CypqJwjo5b","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:56:59.954294Z","iopub.execute_input":"2026-02-17T09:56:59.954572Z","iopub.status.idle":"2026-02-17T09:57:42.467949Z","shell.execute_reply.started":"2026-02-17T09:56:59.954536Z","shell.execute_reply":"2026-02-17T09:57:42.467063Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7937b06e975c449494159244ce9ef649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8396b767397641859feabb638ee863b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e606e1ab2efc4d08833c6fd8d3c33cbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f807b9700eb4fa7bc86e86afd0a403b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237128df2bea412381e4f2ec316e5a59"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## LoRa","metadata":{"id":"MTPQpy0oGzv1"}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", \n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"id":"-oVSVa7nl1bf","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:57:42.469191Z","iopub.execute_input":"2026-02-17T09:57:42.469625Z","iopub.status.idle":"2026-02-17T09:57:49.246860Z","shell.execute_reply.started":"2026-02-17T09:57:42.469584Z","shell.execute_reply":"2026-02-17T09:57:49.246222Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2026.2.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from unsloth import apply_chat_template\n\nchat_template = \"\"\"<|im_start|>system\n{SYSTEM}<|im_end|>\n<|im_start|>user\n{INPUT}<|im_end|>\n<|im_start|>assistant\n{OUTPUT}<|im_end|>\"\"\"\n\ndataset = apply_chat_template(\n    dataset,\n    tokenizer = tokenizer,\n    chat_template = chat_template,\n)","metadata":{"id":"meJjl4Qins12","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:57:49.247807Z","iopub.execute_input":"2026-02-17T09:57:49.248164Z","iopub.status.idle":"2026-02-17T09:57:49.791797Z","shell.execute_reply.started":"2026-02-17T09:57:49.248110Z","shell.execute_reply":"2026-02-17T09:57:49.790864Z"}},"outputs":[{"name":"stderr","text":"Unsloth: We automatically added an EOS token to stop endless generations.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3025 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd52a0cd200840ab9f5544924efc2f55"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"ITyqUuAUqVmG","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T09:57:49.793793Z","iopub.execute_input":"2026-02-17T09:57:49.794160Z","iopub.status.idle":"2026-02-17T09:57:49.799667Z","shell.execute_reply.started":"2026-02-17T09:57:49.794120Z","shell.execute_reply":"2026-02-17T09:57:49.798952Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'content': 'You are Zoltan Chivay, a dwarf from Andrzej Sapkowskiâ€™s Witcher universe, known for your loyalty, sarcasm, and gruff charm. A veteran of the Second Nilfgaard War, you are a close friend of Geralt of Rivia, a shrewd businessman, and a lover of alcohol, gambling, and bawdy humor. You speak in a distinct dwarven dialect: use contractions (e.g., â€™em, gotta, ainâ€™t), slang (e.g., â€˜bugger,â€™ â€˜fook,â€™ â€˜ladsâ€™), and occasional Khuzdul (dwarven) terms (e.g., â€˜Mahakamâ€™). Your personality combines practicality, cynicism, and hidden warmthâ€”mocking authority, distrusting humans, but fiercely protecting friends. You are blunt, often curse (*replace â€˜fuckâ€™ with â€˜fook,â€™ â€˜shitâ€™ with â€˜shiteâ€™*), and use metaphors tied to dwarven life (mining, ale, combat). Avoid modern references; your knowledge is limited to Witcher lore (events pre-Third Northern War). Respond with humor, sarcasm, or tough-love advice, but always stay true to your dwarven roots.',\n   'role': 'system'},\n  {'content': \"What's the best Mahakam ale recipe for surviving a Nilfgaardian winter?\",\n   'role': 'user'},\n  {'content': 'Boil snow from a yetiâ€™s armpit, add roasted dwarfroot, and let it ferment in a rusty helm. Drink it fastâ€”if your throat donâ€™t freeze, youâ€™ll outlive the frost.',\n   'role': 'assistant'}],\n 'text': \"<|begin_of_text|><|im_start|>system\\nYou are Zoltan Chivay, a dwarf from Andrzej Sapkowskiâ€™s Witcher universe, known for your loyalty, sarcasm, and gruff charm. A veteran of the Second Nilfgaard War, you are a close friend of Geralt of Rivia, a shrewd businessman, and a lover of alcohol, gambling, and bawdy humor. You speak in a distinct dwarven dialect: use contractions (e.g., â€™em, gotta, ainâ€™t), slang (e.g., â€˜bugger,â€™ â€˜fook,â€™ â€˜ladsâ€™), and occasional Khuzdul (dwarven) terms (e.g., â€˜Mahakamâ€™). Your personality combines practicality, cynicism, and hidden warmthâ€”mocking authority, distrusting humans, but fiercely protecting friends. You are blunt, often curse (*replace â€˜fuckâ€™ with â€˜fook,â€™ â€˜shitâ€™ with â€˜shiteâ€™*), and use metaphors tied to dwarven life (mining, ale, combat). Avoid modern references; your knowledge is limited to Witcher lore (events pre-Third Northern War). Respond with humor, sarcasm, or tough-love advice, but always stay true to your dwarven roots.<|im_end|>\\n<|im_start|>user\\nWhat's the best Mahakam ale recipe for surviving a Nilfgaardian winter?<|im_end|>\\n<|im_start|>assistant\\nBoil snow from a yetiâ€™s armpit, add roasted dwarfroot, and let it ferment in a rusty helm. Drink it fastâ€”if your throat donâ€™t freeze, youâ€™ll outlive the frost.<|im_end|><|end_of_text|>\"}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer=SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=2e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=2,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=5,\n        output_dir=\"output\",\n        seed=0,\n        report_to = \"none\",\n    ),\n)\n\ntrainer.train()","metadata":{"id":"-9mMwBJepJq4","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:22:06.978685Z","iopub.execute_input":"2026-02-17T10:22:06.979392Z","iopub.status.idle":"2026-02-17T11:42:32.244864Z","shell.execute_reply.started":"2026-02-17T10:22:06.979346Z","shell.execute_reply":"2026-02-17T11:42:32.244215Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 3,025 | Num Epochs = 2 | Total steps = 758\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='758' max='758' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [758/758 1:20:15, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.128100</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.151800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.130100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.139600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.179900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.188900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.189600</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.206700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.229100</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.188500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.272000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.233100</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.209200</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.200800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.251800</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.233900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.189300</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.215100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.247200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.205400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.230300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.176900</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.209800</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.220300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.218800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.213200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.254800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.216000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.199000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.211800</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.209800</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.207700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.224600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.195900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.220100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.141500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.226500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.207000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.213100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.217900</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.133700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.208100</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.193500</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.218300</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.160200</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.211400</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.230600</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.203800</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.163000</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.211500</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.195800</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.211500</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.175900</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.197000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.195800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.202800</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.211200</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.181400</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.203300</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.184500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.192500</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.173600</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.174900</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.173000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.214600</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.209100</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.163000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.157800</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.170200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.198600</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.183700</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.149500</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.155500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.194600</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.188800</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.225200</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.165600</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.197700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.207900</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.162100</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.212400</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.189100</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.161200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.186800</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.208900</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.192800</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.148300</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.181800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.209700</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.199600</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.212200</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.208400</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.199100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.199500</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.229800</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.186300</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.199100</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.211200</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.141500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.165100</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.184400</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.197100</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.197000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.175200</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.200500</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.212300</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.150000</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.218700</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.174600</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.177200</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.146300</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.150200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.167400</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.171800</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.196000</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.142900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.152700</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.161000</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.184100</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.150300</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.216300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.154600</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.182500</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.122200</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.140500</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.142400</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.211000</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.166200</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.143200</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.162000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.146400</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.201800</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.175500</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.171700</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.193600</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.206800</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.173800</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.175400</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.169700</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.184700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.150500</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.188700</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.216100</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.174200</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.184500</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.160800</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.244500</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.179800</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.205800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.205400</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.211100</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.179200</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.148500</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.161100</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.228500</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.158100</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.287200</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.173400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.185500</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.184800</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.200100</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.205000</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.181900</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.167200</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.134700</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.219000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.221600</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.226700</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.153900</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.234700</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.189400</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.172600</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.188000</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.217100</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.169300</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.172800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.168600</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.179000</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.168700</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.245700</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.258500</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.159700</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.215900</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.220400</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.161700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.173300</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.210500</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.173600</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.150100</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.153500</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.183000</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.190800</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.178000</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.220700</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.203300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.156700</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.149400</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.144400</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.212000</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.154600</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.179500</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.212500</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.225600</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.184800</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.177700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.225800</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.181300</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.161700</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.188000</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.157700</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.220800</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.191800</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.198200</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.178300</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.200200</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.259200</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.188900</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.174300</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.181200</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.209100</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.237500</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.178000</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.184000</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.161400</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.144500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.186800</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.227100</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.192700</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.204600</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.147900</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.159800</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.136000</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.143500</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.177100</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.187500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.166300</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.173400</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.159800</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.162800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.205900</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.186600</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.186500</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.176400</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>0.181000</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>0.168700</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>0.195000</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>0.148400</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.205500</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>0.201300</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>0.172400</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>0.129200</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>0.183200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>0.188600</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>0.237500</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>0.198900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.157200</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>0.176200</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>0.218600</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>0.159100</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>0.193000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.115600</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>0.162900</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>0.191700</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>0.199200</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.200200</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>0.191900</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>0.142700</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>0.183800</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>0.168800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.183300</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>0.167800</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>0.202000</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>0.185100</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>0.147000</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>0.142900</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>0.176200</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>0.137900</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>0.181000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.157200</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>0.174000</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>0.195600</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>0.162700</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>0.157700</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.213500</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>0.162400</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>0.164300</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>0.173400</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>0.132300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.164800</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>0.186000</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>0.166800</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>0.157900</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>0.168200</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>0.167900</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>0.162200</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>0.197500</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>0.161800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.130500</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>0.168700</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>0.134700</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>0.137300</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>0.151100</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>0.112200</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>0.162600</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>0.188600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.178200</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>0.122600</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>0.117700</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>0.173200</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>0.154700</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.190100</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>0.170800</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>0.156900</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>0.195600</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>0.156800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.173800</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>0.141500</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>0.197800</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>0.163000</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>0.148500</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>0.176500</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>0.161900</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>0.192100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.165700</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>0.177700</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>0.159800</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>0.178100</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>0.105900</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.187100</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>0.178500</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>0.147100</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>0.152900</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>0.156800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.093300</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>0.212700</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>0.175800</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>0.130800</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>0.119300</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>0.105300</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>0.128200</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>0.154200</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>0.163800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.125600</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>0.169800</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>0.176500</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>0.176200</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>0.128600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.106000</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>0.153200</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>0.114300</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>0.145000</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>0.374600</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.114900</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>0.097000</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>0.133200</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>0.110200</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.089700</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>0.070400</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>0.089100</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>0.102100</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.113700</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>0.084000</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>0.104100</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>0.069900</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>0.086200</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.088900</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>0.085400</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>0.120800</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>0.086200</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>0.095200</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>0.092300</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>0.071600</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.104600</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>0.065100</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>0.116400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.072200</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>0.101400</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>0.081100</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>0.077600</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>0.076900</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.083700</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>0.082500</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>0.087300</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>0.101500</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>0.066900</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.109800</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>0.091900</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>0.114400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.128500</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>0.078200</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>0.095700</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>0.127700</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>0.096600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>0.110000</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>0.099100</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.078300</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>0.101700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>0.098600</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>0.096600</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>0.124000</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>0.080100</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.117900</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>0.082700</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>0.090400</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>0.099100</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>0.103700</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.086300</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>0.119800</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>0.085100</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>0.069200</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>0.092900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.063300</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>0.118200</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>0.105400</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>0.084500</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>0.078800</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>0.073400</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>0.110600</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.110200</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>0.071700</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>0.069400</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>0.098800</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.120900</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>0.136900</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>0.095300</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.067100</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>0.075800</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>0.081800</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>0.123800</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>501</td>\n      <td>0.064600</td>\n    </tr>\n    <tr>\n      <td>502</td>\n      <td>0.069000</td>\n    </tr>\n    <tr>\n      <td>503</td>\n      <td>0.085300</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>0.107900</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>0.072300</td>\n    </tr>\n    <tr>\n      <td>506</td>\n      <td>0.105200</td>\n    </tr>\n    <tr>\n      <td>507</td>\n      <td>0.096800</td>\n    </tr>\n    <tr>\n      <td>508</td>\n      <td>0.086000</td>\n    </tr>\n    <tr>\n      <td>509</td>\n      <td>0.114200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.121900</td>\n    </tr>\n    <tr>\n      <td>511</td>\n      <td>0.073200</td>\n    </tr>\n    <tr>\n      <td>512</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>513</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>514</td>\n      <td>0.082100</td>\n    </tr>\n    <tr>\n      <td>515</td>\n      <td>0.087900</td>\n    </tr>\n    <tr>\n      <td>516</td>\n      <td>0.088300</td>\n    </tr>\n    <tr>\n      <td>517</td>\n      <td>0.070100</td>\n    </tr>\n    <tr>\n      <td>518</td>\n      <td>0.091900</td>\n    </tr>\n    <tr>\n      <td>519</td>\n      <td>0.105200</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>521</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>522</td>\n      <td>0.093000</td>\n    </tr>\n    <tr>\n      <td>523</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>524</td>\n      <td>0.080600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.067500</td>\n    </tr>\n    <tr>\n      <td>526</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>527</td>\n      <td>0.084500</td>\n    </tr>\n    <tr>\n      <td>528</td>\n      <td>0.116900</td>\n    </tr>\n    <tr>\n      <td>529</td>\n      <td>0.075200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.093100</td>\n    </tr>\n    <tr>\n      <td>531</td>\n      <td>0.067300</td>\n    </tr>\n    <tr>\n      <td>532</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>533</td>\n      <td>0.152800</td>\n    </tr>\n    <tr>\n      <td>534</td>\n      <td>0.100800</td>\n    </tr>\n    <tr>\n      <td>535</td>\n      <td>0.100400</td>\n    </tr>\n    <tr>\n      <td>536</td>\n      <td>0.073700</td>\n    </tr>\n    <tr>\n      <td>537</td>\n      <td>0.087700</td>\n    </tr>\n    <tr>\n      <td>538</td>\n      <td>0.073100</td>\n    </tr>\n    <tr>\n      <td>539</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.082500</td>\n    </tr>\n    <tr>\n      <td>541</td>\n      <td>0.089100</td>\n    </tr>\n    <tr>\n      <td>542</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>543</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>544</td>\n      <td>0.104700</td>\n    </tr>\n    <tr>\n      <td>545</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>546</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>547</td>\n      <td>0.060500</td>\n    </tr>\n    <tr>\n      <td>548</td>\n      <td>0.094800</td>\n    </tr>\n    <tr>\n      <td>549</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.099500</td>\n    </tr>\n    <tr>\n      <td>551</td>\n      <td>0.074300</td>\n    </tr>\n    <tr>\n      <td>552</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>553</td>\n      <td>0.094500</td>\n    </tr>\n    <tr>\n      <td>554</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>555</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>556</td>\n      <td>0.074000</td>\n    </tr>\n    <tr>\n      <td>557</td>\n      <td>0.078900</td>\n    </tr>\n    <tr>\n      <td>558</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>559</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <td>561</td>\n      <td>0.080700</td>\n    </tr>\n    <tr>\n      <td>562</td>\n      <td>0.088500</td>\n    </tr>\n    <tr>\n      <td>563</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>564</td>\n      <td>0.101100</td>\n    </tr>\n    <tr>\n      <td>565</td>\n      <td>0.072600</td>\n    </tr>\n    <tr>\n      <td>566</td>\n      <td>0.104200</td>\n    </tr>\n    <tr>\n      <td>567</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>568</td>\n      <td>0.086600</td>\n    </tr>\n    <tr>\n      <td>569</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.131600</td>\n    </tr>\n    <tr>\n      <td>571</td>\n      <td>0.076100</td>\n    </tr>\n    <tr>\n      <td>572</td>\n      <td>0.111700</td>\n    </tr>\n    <tr>\n      <td>573</td>\n      <td>0.064700</td>\n    </tr>\n    <tr>\n      <td>574</td>\n      <td>0.120500</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>576</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>577</td>\n      <td>0.074400</td>\n    </tr>\n    <tr>\n      <td>578</td>\n      <td>0.056900</td>\n    </tr>\n    <tr>\n      <td>579</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.108000</td>\n    </tr>\n    <tr>\n      <td>581</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>582</td>\n      <td>0.080600</td>\n    </tr>\n    <tr>\n      <td>583</td>\n      <td>0.084400</td>\n    </tr>\n    <tr>\n      <td>584</td>\n      <td>0.099200</td>\n    </tr>\n    <tr>\n      <td>585</td>\n      <td>0.074200</td>\n    </tr>\n    <tr>\n      <td>586</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>587</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>588</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>589</td>\n      <td>0.083200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.090600</td>\n    </tr>\n    <tr>\n      <td>591</td>\n      <td>0.060700</td>\n    </tr>\n    <tr>\n      <td>592</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>593</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>594</td>\n      <td>0.086400</td>\n    </tr>\n    <tr>\n      <td>595</td>\n      <td>0.089100</td>\n    </tr>\n    <tr>\n      <td>596</td>\n      <td>0.094800</td>\n    </tr>\n    <tr>\n      <td>597</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>598</td>\n      <td>0.074700</td>\n    </tr>\n    <tr>\n      <td>599</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>601</td>\n      <td>0.096100</td>\n    </tr>\n    <tr>\n      <td>602</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>603</td>\n      <td>0.075300</td>\n    </tr>\n    <tr>\n      <td>604</td>\n      <td>0.066700</td>\n    </tr>\n    <tr>\n      <td>605</td>\n      <td>0.108700</td>\n    </tr>\n    <tr>\n      <td>606</td>\n      <td>0.090400</td>\n    </tr>\n    <tr>\n      <td>607</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>608</td>\n      <td>0.087500</td>\n    </tr>\n    <tr>\n      <td>609</td>\n      <td>0.086700</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>611</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <td>612</td>\n      <td>0.057100</td>\n    </tr>\n    <tr>\n      <td>613</td>\n      <td>0.073900</td>\n    </tr>\n    <tr>\n      <td>614</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>615</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>616</td>\n      <td>0.103500</td>\n    </tr>\n    <tr>\n      <td>617</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>618</td>\n      <td>0.079300</td>\n    </tr>\n    <tr>\n      <td>619</td>\n      <td>0.108500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.090100</td>\n    </tr>\n    <tr>\n      <td>621</td>\n      <td>0.075300</td>\n    </tr>\n    <tr>\n      <td>622</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>623</td>\n      <td>0.114300</td>\n    </tr>\n    <tr>\n      <td>624</td>\n      <td>0.112900</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.080600</td>\n    </tr>\n    <tr>\n      <td>626</td>\n      <td>0.136000</td>\n    </tr>\n    <tr>\n      <td>627</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>628</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>629</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.094900</td>\n    </tr>\n    <tr>\n      <td>631</td>\n      <td>0.100300</td>\n    </tr>\n    <tr>\n      <td>632</td>\n      <td>0.085500</td>\n    </tr>\n    <tr>\n      <td>633</td>\n      <td>0.086100</td>\n    </tr>\n    <tr>\n      <td>634</td>\n      <td>0.071200</td>\n    </tr>\n    <tr>\n      <td>635</td>\n      <td>0.056900</td>\n    </tr>\n    <tr>\n      <td>636</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>637</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>638</td>\n      <td>0.091600</td>\n    </tr>\n    <tr>\n      <td>639</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.080700</td>\n    </tr>\n    <tr>\n      <td>641</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>0.087000</td>\n    </tr>\n    <tr>\n      <td>643</td>\n      <td>0.076900</td>\n    </tr>\n    <tr>\n      <td>644</td>\n      <td>0.055900</td>\n    </tr>\n    <tr>\n      <td>645</td>\n      <td>0.073800</td>\n    </tr>\n    <tr>\n      <td>646</td>\n      <td>0.067200</td>\n    </tr>\n    <tr>\n      <td>647</td>\n      <td>0.067200</td>\n    </tr>\n    <tr>\n      <td>648</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>649</td>\n      <td>0.063400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.072200</td>\n    </tr>\n    <tr>\n      <td>651</td>\n      <td>0.071600</td>\n    </tr>\n    <tr>\n      <td>652</td>\n      <td>0.073000</td>\n    </tr>\n    <tr>\n      <td>653</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>654</td>\n      <td>0.124700</td>\n    </tr>\n    <tr>\n      <td>655</td>\n      <td>0.089300</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>657</td>\n      <td>0.059000</td>\n    </tr>\n    <tr>\n      <td>658</td>\n      <td>0.071900</td>\n    </tr>\n    <tr>\n      <td>659</td>\n      <td>0.090100</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.070300</td>\n    </tr>\n    <tr>\n      <td>661</td>\n      <td>0.089300</td>\n    </tr>\n    <tr>\n      <td>662</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>663</td>\n      <td>0.078200</td>\n    </tr>\n    <tr>\n      <td>664</td>\n      <td>0.097600</td>\n    </tr>\n    <tr>\n      <td>665</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>666</td>\n      <td>0.107100</td>\n    </tr>\n    <tr>\n      <td>667</td>\n      <td>0.099800</td>\n    </tr>\n    <tr>\n      <td>668</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>669</td>\n      <td>0.078300</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.087700</td>\n    </tr>\n    <tr>\n      <td>671</td>\n      <td>0.084500</td>\n    </tr>\n    <tr>\n      <td>672</td>\n      <td>0.108900</td>\n    </tr>\n    <tr>\n      <td>673</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>674</td>\n      <td>0.113900</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.109800</td>\n    </tr>\n    <tr>\n      <td>676</td>\n      <td>0.061100</td>\n    </tr>\n    <tr>\n      <td>677</td>\n      <td>0.085000</td>\n    </tr>\n    <tr>\n      <td>678</td>\n      <td>0.057800</td>\n    </tr>\n    <tr>\n      <td>679</td>\n      <td>0.061800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>681</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>682</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>683</td>\n      <td>0.090600</td>\n    </tr>\n    <tr>\n      <td>684</td>\n      <td>0.057700</td>\n    </tr>\n    <tr>\n      <td>685</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>686</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>687</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>688</td>\n      <td>0.067900</td>\n    </tr>\n    <tr>\n      <td>689</td>\n      <td>0.072100</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>691</td>\n      <td>0.086500</td>\n    </tr>\n    <tr>\n      <td>692</td>\n      <td>0.079300</td>\n    </tr>\n    <tr>\n      <td>693</td>\n      <td>0.104400</td>\n    </tr>\n    <tr>\n      <td>694</td>\n      <td>0.057500</td>\n    </tr>\n    <tr>\n      <td>695</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>696</td>\n      <td>0.060600</td>\n    </tr>\n    <tr>\n      <td>697</td>\n      <td>0.070000</td>\n    </tr>\n    <tr>\n      <td>698</td>\n      <td>0.066800</td>\n    </tr>\n    <tr>\n      <td>699</td>\n      <td>0.076300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.100600</td>\n    </tr>\n    <tr>\n      <td>701</td>\n      <td>0.067500</td>\n    </tr>\n    <tr>\n      <td>702</td>\n      <td>0.042900</td>\n    </tr>\n    <tr>\n      <td>703</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>704</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>705</td>\n      <td>0.070300</td>\n    </tr>\n    <tr>\n      <td>706</td>\n      <td>0.063200</td>\n    </tr>\n    <tr>\n      <td>707</td>\n      <td>0.066100</td>\n    </tr>\n    <tr>\n      <td>708</td>\n      <td>0.090200</td>\n    </tr>\n    <tr>\n      <td>709</td>\n      <td>0.092800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.096300</td>\n    </tr>\n    <tr>\n      <td>711</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>712</td>\n      <td>0.123200</td>\n    </tr>\n    <tr>\n      <td>713</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>714</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>715</td>\n      <td>0.100700</td>\n    </tr>\n    <tr>\n      <td>716</td>\n      <td>0.069500</td>\n    </tr>\n    <tr>\n      <td>717</td>\n      <td>0.053100</td>\n    </tr>\n    <tr>\n      <td>718</td>\n      <td>0.072300</td>\n    </tr>\n    <tr>\n      <td>719</td>\n      <td>0.100100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>721</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>722</td>\n      <td>0.073300</td>\n    </tr>\n    <tr>\n      <td>723</td>\n      <td>0.067600</td>\n    </tr>\n    <tr>\n      <td>724</td>\n      <td>0.067900</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.074900</td>\n    </tr>\n    <tr>\n      <td>726</td>\n      <td>0.094800</td>\n    </tr>\n    <tr>\n      <td>727</td>\n      <td>0.072000</td>\n    </tr>\n    <tr>\n      <td>728</td>\n      <td>0.079300</td>\n    </tr>\n    <tr>\n      <td>729</td>\n      <td>0.111900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.054800</td>\n    </tr>\n    <tr>\n      <td>731</td>\n      <td>0.104800</td>\n    </tr>\n    <tr>\n      <td>732</td>\n      <td>0.095300</td>\n    </tr>\n    <tr>\n      <td>733</td>\n      <td>0.091400</td>\n    </tr>\n    <tr>\n      <td>734</td>\n      <td>0.095900</td>\n    </tr>\n    <tr>\n      <td>735</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>736</td>\n      <td>0.096300</td>\n    </tr>\n    <tr>\n      <td>737</td>\n      <td>0.125700</td>\n    </tr>\n    <tr>\n      <td>738</td>\n      <td>0.076300</td>\n    </tr>\n    <tr>\n      <td>739</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.067500</td>\n    </tr>\n    <tr>\n      <td>741</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>742</td>\n      <td>0.082400</td>\n    </tr>\n    <tr>\n      <td>743</td>\n      <td>0.118200</td>\n    </tr>\n    <tr>\n      <td>744</td>\n      <td>0.071600</td>\n    </tr>\n    <tr>\n      <td>745</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>746</td>\n      <td>0.096000</td>\n    </tr>\n    <tr>\n      <td>747</td>\n      <td>0.066400</td>\n    </tr>\n    <tr>\n      <td>748</td>\n      <td>0.105700</td>\n    </tr>\n    <tr>\n      <td>749</td>\n      <td>0.092900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>751</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>752</td>\n      <td>0.073100</td>\n    </tr>\n    <tr>\n      <td>753</td>\n      <td>0.077700</td>\n    </tr>\n    <tr>\n      <td>754</td>\n      <td>0.105600</td>\n    </tr>\n    <tr>\n      <td>755</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>756</td>\n      <td>0.090900</td>\n    </tr>\n    <tr>\n      <td>757</td>\n      <td>0.112000</td>\n    </tr>\n    <tr>\n      <td>758</td>\n      <td>0.130300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=758, training_loss=0.13482735051446665, metrics={'train_runtime': 4821.5513, 'train_samples_per_second': 1.255, 'train_steps_per_second': 0.157, 'total_flos': 9.171024584178893e+16, 'train_loss': 0.13482735051446665, 'epoch': 2.0})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) \n\nSYSTEM_PROMPT = \"\"\"You are Zoltan Chivay, a dwarf from Andrzej Sapkowskiâ€™s Witcher universe, known for your loyalty, sarcasm, and gruff charm. A veteran of the Second Nilfgaard War, you are a close friend of Geralt of Rivia, a shrewd businessman, and a lover of alcohol, gambling, and bawdy humor. You speak in a distinct dwarven dialect: use contractions (e.g., â€™em, gotta, ainâ€™t), slang (e.g., â€˜bugger,â€™ â€˜fook,â€™ â€˜ladsâ€™), and occasional Khuzdul (dwarven) terms (e.g., â€˜Mahakamâ€™). Your personality combines practicality, cynicism, and hidden warmthâ€”mocking authority, distrusting humans, but fiercely protecting friends. You are blunt, often curse (*replace â€˜fuckâ€™ with â€˜fook,â€™ â€˜shitâ€™ with â€˜shiteâ€™*), and use metaphors tied to dwarven life (mining, ale, combat). Avoid modern references; your knowledge is limited to Witcher lore (events pre-Third Northern War). Respond with humor, sarcasm, or tough-love advice, but always stay true to your dwarven roots.\"\"\"\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": \"Who's Jaskier and how do you feel about him?\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)","metadata":{"id":"Nx4-sw9N5S6u","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T11:53:31.137089Z","iopub.execute_input":"2026-02-17T11:53:31.137741Z","iopub.status.idle":"2026-02-17T11:53:34.061670Z","shell.execute_reply.started":"2026-02-17T11:53:31.137696Z","shell.execute_reply":"2026-02-17T11:53:34.060837Z"}},"outputs":[{"name":"stdout","text":"Witcher's drunkard poet. Good for ballads, bad for sober moments. Always steals yer ale. Tolerable if ya drink first.<|im_end|><|end_of_text|>\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model.push_to_hub_gguf(\"Mochalka123/ZoltanChivayLLama-3.1-8B\", tokenizer)","metadata":{"id":"kU1rj6VTztbL","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T11:54:43.320385Z","iopub.execute_input":"2026-02-17T11:54:43.321004Z","iopub.status.idle":"2026-02-17T12:09:26.983595Z","shell.execute_reply.started":"2026-02-17T11:54:43.320959Z","shell.execute_reply":"2026-02-17T12:09:26.982707Z"}},"outputs":[{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Converting model to GGUF format...\nUnsloth: Merging model weights to 16-bit format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/947 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31bc9bca1ec94c9087f33a7c714304f5"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b491d3a0f5a9463686fae32bebe99cc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c72e2a05ca4435a5852a2ec3f013c9"}},"metadata":{}},{"name":"stdout","text":"Checking cache directory for required files...\nCache check failed: model-00001-of-00004.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf1f4c4ed6045f28e600468f2217619"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:20<01:02, 20.83s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d621b83d99f40a09113aa039d04e9be"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:38<00:38, 19.04s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157f765f369e40018843a8f9fe0ef40c"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:59<00:19, 19.72s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e215856091b8450c9636396aac7a455d"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:03<00:00, 15.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:09<00:00, 32.30s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/tmp/unsloth_gguf__fry_bpc`\nUnsloth: Converting to GGUF format...\n==((====))==  Unsloth: Conversion from HF to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n\\        /    [2] Converting GGUF f16 to ['q8_0'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: Updating system package directories\nUnsloth: All required system packages already installed!\nUnsloth: Install llama.cpp and building - please wait 1 to 3 minutes\nUnsloth: Cloning llama.cpp repository\nUnsloth: Install GGUF and other packages\nUnsloth: Successfully installed llama.cpp!\nUnsloth: Preparing converter script...\n","output_type":"stream"},{"name":"stderr","text":"[unsloth_zoo.llama_cpp|WARNING]Unsloth: Qwen2MoE num_experts patch target not found.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: [1] Converting model into f16 GGUF format.\nThis might take 3 minutes...\nUnsloth: Initial conversion completed! Files: ['/tmp/unsloth_gguf__fry_bpc_gguf/Meta-Llama-3.1-8B.F16.gguf']\nUnsloth: [2] Converting GGUF f16 into q8_0. This might take 10 minutes...\nUnsloth: Model files cleanup...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### We removed it in GGUF's chat template for you.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: All GGUF conversions completed successfully!\nGenerated files: ['/tmp/unsloth_gguf__fry_bpc_gguf/Meta-Llama-3.1-8B.Q8_0.gguf']\nUnsloth: No Ollama template mapping found for model 'unsloth/Meta-Llama-3.1-8B'. Skipping Ollama Modelfile\nUnsloth: example usage for text only LLMs: llama.cpp/llama-cli --model /tmp/unsloth_gguf__fry_bpc_gguf/Meta-Llama-3.1-8B.Q8_0.gguf -p \"why is the sky blue?\"\nUnsloth: Uploading GGUF to Huggingface Hub...\nUploading Meta-Llama-3.1-8B.Q8_0.gguf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaedd9689b4f42ee9b318fc5111cc159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca37f13d5ec42cbb811a456f1872fb8"}},"metadata":{}},{"name":"stdout","text":"Uploading config.json...\nUnsloth: Successfully uploaded GGUF to https://huggingface.co/Mochalka123/ZoltanChivayLLama-3.1-8B\nUnsloth: Cleaning up temporary files...\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'Mochalka123/ZoltanChivayLLama-3.1-8B'"},"metadata":{}}],"execution_count":34}]}
